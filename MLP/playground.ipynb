{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "words = open('../data/names.txt', 'r').read().splitlines()\n",
    "\n",
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11170"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbr_neurons = 256\n",
    "vocab_size = 27\n",
    "emb_dim = 5\n",
    "block_size = 3 # context window size\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) \n",
    "C = torch.randn((vocab_size, emb_dim), generator=g)\n",
    "W1 = torch.randn((emb_dim*block_size, nbr_neurons), generator=g)\n",
    "b1 = torch.randn(nbr_neurons, generator=g)\n",
    "W2 = torch.randn((nbr_neurons, vocab_size), generator=g)\n",
    "b2 = torch.randn(vocab_size, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "sum(p.nelement() for p in parameters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "def build_dataset(words, block_size=block_size):  \n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  #print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "        p.requires_grad = True \n",
    "stepi, lossi, Ci = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2476, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n = 50000\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    # minibatch construct \n",
    "    ixs = torch.randint(0, Xtr.shape[0], (batch_size,)) \n",
    "\n",
    "    # Forward pass \n",
    "    emb = C[Xtr[ixs]] \n",
    "    h = torch.tanh(emb.view(-1,block_size*emb_dim) @ W1 + b1) \n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Ytr[ixs]) \n",
    "\n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    #l = 0.1 if i < 300000 else 0.01\n",
    "    l = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -l * p.grad \n",
    "\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.item())\n",
    "    C_clone = C.detach().clone()\n",
    "    Ci.append(C_clone)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev :  tensor(2.1341, grad_fn=<NllLossBackward0>)\n",
      "test :  tensor(2.1295, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def model_test(X,Y):\n",
    "    emb = C[X] \n",
    "    h = torch.tanh(emb.view(-1,block_size*emb_dim) @ W1 + b1) \n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Y) \n",
    "    return loss\n",
    "\n",
    "ldev = model_test(Xdev, Ydev)\n",
    "ltest = model_test(Xte, Yte)\n",
    "print(\"dev : \", ldev)\n",
    "print(\"test : \", ltest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: lr=0.01, nbr_neurons=128, emb_dim=8, block_size=4, dev_loss=2.340148687362671\n",
      "Trial 2: lr=0.01, nbr_neurons=256, emb_dim=4, block_size=3, dev_loss=2.4161903858184814\n",
      "Trial 3: lr=0.01, nbr_neurons=256, emb_dim=4, block_size=4, dev_loss=2.387986898422241\n",
      "Trial 4: lr=0.01, nbr_neurons=256, emb_dim=12, block_size=5, dev_loss=2.42612361907959\n",
      "Trial 5: lr=0.01, nbr_neurons=512, emb_dim=4, block_size=3, dev_loss=2.439927339553833\n",
      "Best configuration: {'lr': 0.01, 'nbr_neurons': 128, 'emb_dim': 8, 'block_size': 4, 'dev_loss': 2.340148687362671, 'C': tensor([[-3.9578e-02, -5.5522e-01, -6.1088e-01,  2.1530e-01,  3.1879e-01,\n",
      "          3.0767e-01,  1.4910e-01,  3.2984e-01],\n",
      "        [-3.4276e-01,  9.4620e-02, -1.0977e-01, -7.0507e-02, -1.9587e-01,\n",
      "         -5.2406e-01,  2.5951e-01,  2.9322e-01],\n",
      "        [-3.1222e-01,  5.8283e-02, -2.3211e-01, -4.4966e-02, -2.9246e-01,\n",
      "         -2.7495e-01,  1.5838e-01,  4.2476e-01],\n",
      "        [-3.2412e-01,  6.4662e-02, -2.2934e-01, -7.6737e-02, -1.0713e-01,\n",
      "         -1.7608e-01,  2.6146e-01,  3.8156e-01],\n",
      "        [-2.5127e-01,  6.1031e-02, -2.9189e-01, -5.5730e-02, -2.5051e-01,\n",
      "         -2.4955e-01,  1.9608e-01,  3.7837e-01],\n",
      "        [-2.2702e-01,  7.8744e-02, -7.8383e-02, -1.5354e-01, -1.5373e-01,\n",
      "         -5.0104e-01,  2.2960e-01,  2.5752e-01],\n",
      "        [-2.9357e-01,  5.6714e-02, -2.6887e-01, -6.1372e-02, -2.3636e-01,\n",
      "         -2.6263e-01,  2.0397e-01,  3.8848e-01],\n",
      "        [-2.7168e-01,  1.1744e-01, -1.3109e-01, -8.4781e-02, -2.1543e-01,\n",
      "         -2.4721e-01,  2.2596e-01,  3.9167e-01],\n",
      "        [-2.2658e-01, -1.8546e-02, -2.8502e-01, -7.8010e-02, -1.9377e-01,\n",
      "         -2.4236e-01,  1.8763e-01,  3.7215e-01],\n",
      "        [-1.9208e-01,  1.6572e-01, -1.0595e-01, -7.4060e-02, -1.2221e-01,\n",
      "         -4.1341e-01,  2.3121e-01,  2.6309e-01],\n",
      "        [-1.6823e-01,  2.7804e-02, -2.7374e-01, -8.9923e-02, -1.4022e-01,\n",
      "         -2.0699e-01,  1.9745e-01,  4.1515e-01],\n",
      "        [-2.6584e-01,  3.7573e-02, -2.9133e-01, -9.0298e-02, -1.7510e-01,\n",
      "         -2.2998e-01,  1.8852e-01,  3.7684e-01],\n",
      "        [-7.0584e-02,  1.0773e-02, -3.2641e-01, -1.9071e-01, -3.5586e-01,\n",
      "         -2.4099e-01,  1.4530e-01,  4.0331e-01],\n",
      "        [-2.7088e-01,  4.8117e-03, -4.1711e-01,  9.5665e-02, -3.6135e-01,\n",
      "         -1.3450e-01,  2.7320e-01,  4.6024e-01],\n",
      "        [-2.3794e-01, -4.6576e-03, -3.9256e-01,  9.0757e-02, -2.8280e-01,\n",
      "         -2.8135e-01,  2.7252e-01,  3.5833e-01],\n",
      "        [-2.7494e-01,  1.0902e-01, -4.2938e-02, -4.5903e-02, -1.6066e-01,\n",
      "         -4.8407e-01,  2.4037e-01,  2.3083e-01],\n",
      "        [-3.3649e-01,  1.1811e-01, -2.2397e-01, -5.5745e-02, -1.7313e-01,\n",
      "         -2.1349e-01,  1.9460e-01,  4.1688e-01],\n",
      "        [ 4.6618e-01, -3.3302e-02,  1.8416e-01, -1.8267e+00,  1.7234e-01,\n",
      "          1.7916e-02,  2.1905e-01,  2.5257e-01],\n",
      "        [ 7.4276e-02,  1.1502e-03, -6.0920e-01,  1.6173e-01, -7.9611e-01,\n",
      "         -1.6872e-01,  3.4790e-01,  4.9646e-01],\n",
      "        [-3.0456e-01,  9.0666e-02, -2.3797e-01, -1.2235e-01, -7.6201e-02,\n",
      "         -2.7927e-01,  2.2808e-01,  3.5260e-01],\n",
      "        [-3.0550e-01,  1.1283e-01, -2.4700e-01, -1.1629e-01, -1.1144e-01,\n",
      "         -2.4964e-01,  1.9493e-01,  3.8837e-01],\n",
      "        [-2.6739e-01,  1.5208e-01, -6.2459e-02, -6.3343e-02, -1.2440e-01,\n",
      "         -4.5037e-01,  1.9254e-01,  2.7892e-01],\n",
      "        [-1.5456e-01,  3.0508e-02, -3.3355e-01, -1.2308e-01, -3.3098e-01,\n",
      "         -1.3095e-01,  2.1234e-01,  4.0908e-01],\n",
      "        [-2.1509e-01,  4.1915e-02, -3.4258e-01,  3.2435e-02, -3.0830e-01,\n",
      "         -1.8222e-01,  1.7865e-01,  4.4978e-01],\n",
      "        [-1.7085e-01, -5.4584e-02, -3.2574e-01, -1.4042e-01, -1.4281e-01,\n",
      "         -1.8308e-01,  1.8816e-01,  2.8578e-01],\n",
      "        [-1.6310e-01,  7.1129e-02, -1.0187e-01, -2.3903e-02, -8.6775e-02,\n",
      "         -3.3595e-01,  2.3274e-01,  2.1600e-01],\n",
      "        [-1.8814e-01, -9.7247e-04, -3.0864e-01, -9.8976e-02, -2.2687e-01,\n",
      "         -2.3103e-01,  2.0015e-01,  4.2858e-01]], requires_grad=True), 'W1': tensor([[ 0.9500, -0.7934, -0.5178,  ...,  1.7569,  0.9976, -0.2953],\n",
      "        [-0.5193,  1.6447, -0.5391,  ...,  0.0528,  0.7049, -1.6064],\n",
      "        [ 1.2407,  1.3935,  1.1368,  ..., -1.4236,  0.8919, -1.0289],\n",
      "        ...,\n",
      "        [-0.1897, -0.7650,  0.6337,  ...,  0.5648, -1.2682,  0.0266],\n",
      "        [-0.6778, -1.3845,  1.2483,  ...,  0.6080,  0.9373, -0.9191],\n",
      "        [ 1.3928,  2.0680, -0.4592,  ...,  0.9787,  1.2676,  0.5622]],\n",
      "       requires_grad=True), 'b1': tensor([ 0.7816, -0.9798, -0.7412, -0.6866, -0.4499, -1.4557, -2.5710,  0.6267,\n",
      "        -2.1947, -0.2569,  0.4820, -0.7232,  0.3341, -0.2488,  0.1142,  1.6577,\n",
      "         0.4160, -1.8497,  1.0250,  0.2131,  0.5124,  0.4511,  0.3483,  0.0711,\n",
      "        -0.1563, -0.2388, -2.5443,  1.6941,  0.0107, -0.4199, -0.5735, -0.0995,\n",
      "         0.2490,  0.1594,  0.2230, -0.6140,  0.8363,  0.5641,  0.3992,  1.7301,\n",
      "        -0.1180,  0.0614,  0.0963, -0.3151, -0.3345, -0.5840,  0.2155, -0.0673,\n",
      "        -0.5811, -0.6321,  1.5193,  0.5294, -1.6065,  0.5269,  1.2349, -2.6196,\n",
      "         0.4973,  0.6376,  0.1236, -0.6428,  0.4551,  0.4846,  0.0980, -1.2776,\n",
      "         1.7821,  0.7323, -1.1063, -0.4339,  0.9876,  0.2183, -1.5451, -0.3522,\n",
      "        -0.7795,  0.0365,  0.4853, -0.2344,  0.8664,  1.1787,  0.9003,  1.0434,\n",
      "        -0.1970,  1.8154,  0.4933, -0.4094, -0.0497,  1.8381,  2.1422, -0.5691,\n",
      "        -0.1699, -0.9153,  0.2134,  1.1235,  0.1366,  0.4894,  0.8185, -0.5093,\n",
      "        -1.7162,  0.8185,  0.1349,  0.7112,  0.8215,  0.4957, -1.0523, -1.5113,\n",
      "         1.0934, -0.7128,  0.2874,  1.4163, -0.0827,  1.2923,  0.8823,  0.8718,\n",
      "         0.2672,  0.9581, -1.1028,  0.3546,  1.3837, -0.5989, -1.5834, -0.1395,\n",
      "        -1.5109,  0.2532, -0.4596, -0.4294,  0.2657, -0.7999, -0.2386, -0.6107],\n",
      "       requires_grad=True), 'W2': tensor([[ 0.5852,  0.4400,  0.4628,  ...,  0.7040,  1.0602,  0.3235],\n",
      "        [-0.8621,  0.3078,  1.1181,  ..., -1.3133, -0.1735, -0.7008],\n",
      "        [ 0.3967,  0.4970, -1.3287,  ...,  2.7150,  0.1235, -1.1443],\n",
      "        ...,\n",
      "        [ 0.3180, -0.2818,  1.6537,  ...,  2.6159, -0.8382,  0.2874],\n",
      "        [ 0.6065,  0.2529,  0.3604,  ..., -1.8300,  0.4166,  0.0676],\n",
      "        [ 0.7408, -0.5086, -1.1603,  ..., -0.2977, -0.2032,  0.1012]],\n",
      "       requires_grad=True), 'b2': tensor([ 0.8284,  0.4720,  0.1641,  0.6991, -1.4881,  0.1248, -1.4736, -1.0508,\n",
      "        -1.7289, -1.4018, -0.5342, -0.1989, -0.4894,  0.4860, -0.2762, -0.5301,\n",
      "        -0.8654, -2.1151,  0.6110,  0.7232,  0.1012, -2.4406, -1.0799, -0.5513,\n",
      "         0.6699, -2.4279, -0.8721], requires_grad=True)}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# DÃ©finition des espaces de recherche\n",
    "nbr_neurons_choices = [128, 256, 512]\n",
    "emb_dim_choices = [4, 8, 12]       \n",
    "block_size_choices = [3, 4, 5]      \n",
    "\n",
    "n_trials = 5      \n",
    "n_iter = 50000     \n",
    "\n",
    "best_loss = float('inf')\n",
    "best_config = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    nbr_neurons_trial = random.choice(nbr_neurons_choices)\n",
    "    emb_dim_trial = random.choice(emb_dim_choices)\n",
    "    block_size_trial = random.choice(block_size_choices)\n",
    "\n",
    "    Xtr, Ytr = build_dataset(words[:n1], block_size_trial)\n",
    "    Xdev, Ydev = build_dataset(words[n1:n2], block_size_trial)\n",
    "    Xte, Yte = build_dataset(words[n2:], block_size_trial)\n",
    "    \n",
    "    \n",
    "    C_trial = torch.randn((vocab_size, emb_dim_trial), generator=g, requires_grad=True)\n",
    "    W1_trial = torch.randn((block_size_trial * emb_dim_trial, nbr_neurons_trial), generator=g, requires_grad=True)\n",
    "    b1_trial = torch.randn(nbr_neurons_trial, generator=g, requires_grad=True)\n",
    "    W2_trial = torch.randn((nbr_neurons_trial, vocab_size), generator=g, requires_grad=True)\n",
    "    b2_trial = torch.randn(vocab_size, generator=g, requires_grad=True)\n",
    "    \n",
    "    parameters_trial = [C_trial, W1_trial, b1_trial, W2_trial, b2_trial]\n",
    "    \n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        lr = 0.1 if n_iter < 0.8*n_iter else 0.01\n",
    "        \n",
    "        ixs = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        \n",
    "        # Forward pass\n",
    "        emb = C_trial[Xtr[ixs]]\n",
    "        h = torch.tanh(emb.view(-1, block_size_trial * emb_dim_trial) @ W1_trial + b1_trial)\n",
    "        logits = h @ W2_trial + b2_trial\n",
    "        loss = F.cross_entropy(logits, Ytr[ixs])\n",
    "        \n",
    "        # Backward pass\n",
    "        for p in parameters_trial:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update\n",
    "        with torch.no_grad():\n",
    "            for p in parameters_trial:\n",
    "                p.add_(-lr * p.grad)\n",
    "    \n",
    "    # eval \n",
    "    emb_dev = C_trial[Xdev]\n",
    "    h_dev = torch.tanh(emb_dev.view(-1, block_size_trial * emb_dim_trial) @ W1_trial + b1_trial)\n",
    "    logits_dev = h_dev @ W2_trial + b2_trial\n",
    "    dev_loss = F.cross_entropy(logits_dev, Ydev)\n",
    "    \n",
    "    print(f\"Trial {trial+1}: lr={lr}, \"\n",
    "          f\"nbr_neurons={nbr_neurons_trial}, \"\n",
    "          f\"emb_dim={emb_dim_trial}, block_size={block_size_trial}, \"\n",
    "          f\"dev_loss={dev_loss.item()}\")\n",
    "    \n",
    "    # Meilleure configuration\n",
    "    if dev_loss.item() < best_loss:\n",
    "        best_loss = dev_loss.item()\n",
    "        best_config = {\n",
    "            'nbr_neurons': nbr_neurons_trial,\n",
    "            'emb_dim': emb_dim_trial,\n",
    "            'block_size': block_size_trial,\n",
    "            'dev_loss': best_loss,\n",
    "            'C': C_trial,\n",
    "            'W1': W1_trial,\n",
    "            'b1': b1_trial,\n",
    "            'W2': W2_trial,\n",
    "            'b2': b2_trial,\n",
    "        }\n",
    "\n",
    "print(\"Best configuration:\", best_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
